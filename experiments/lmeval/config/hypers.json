{
    "Qwen/Qwen3-8B": {
        "model": "Qwen/Qwen3-8B",
        "hypers":[
            {
                "cache_size": 512,
                "window_size": 32,
                "tau1": 7.774380648611771,
                "tau2": 5.407015096778621,
                "tau3": 5.527702582901318,
                "gamma": 263.8108280337562
            },
            {
                "cache_size": 1024,
                "window_size": 32,
                "tau1": 7.774380648611771,
                "tau2": 5.407015096778621,
                "tau3": 5.527702582901318,
                "gamma": 263.8108280337562
            },
            {
                "cache_size": 2048,
                "window_size": 32,
                "tau1": 7.774380648611771,
                "tau2": 5.407015096778621,
                "tau3": 5.527702582901318,
                "gamma": 263.8108280337562
            }
        ]
    },
    "meta-llama/Meta-Llama-3.1-8B-Instruct": {
        "model": "meta-llama/Meta-Llama-3.1-8B-Instruct",
        "hypers":[
            {
                "cache_size": 512,
                "window_size": 32,
                "tau1": 7.774380648611771,
                "tau2": 5.407015096778621,
                "tau3": 5.527702582901318,
                "gamma": 263.8108280337562
            },
            {
                "cache_size": 1024,
                "window_size": 32,
                "tau1": 7.774380648611771,
                "tau2": 5.407015096778621,
                "tau3": 5.527702582901318,
                "gamma": 263.8108280337562
            },
            {
                "cache_size": 2048,
                "window_size": 32,
                "tau1": 7.774380648611771,
                "tau2": 5.407015096778621,
                "tau3": 5.527702582901318,
                "gamma": 263.8108280337562
            }
        ]
    }
}